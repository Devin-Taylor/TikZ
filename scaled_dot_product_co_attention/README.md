# Scaled Dot Product Co-Attention

## Notes

Scaled dot product co-attention algorithm, drawn as a comparison to scaled dot product attention from the Transformer paper.

## Output

![sdpca](https://www.dropbox.com/s/kr3z5j4xsg8ggbz/sdpca.png?raw=1)
