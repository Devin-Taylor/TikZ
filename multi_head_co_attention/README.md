# Multi-Head Co-Attention

## Notes

Multi-head co-attention algorithm, drawn as a comparison to multi-head attention from the Transformer paper.

## Output

![mhca](https://www.dropbox.com/s/xh83y40hzx31p40/mhca.png?raw=1)
